{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98053e24-a7ca-4d39-8e20-2c766cc2f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Train script (train + val) for PONE ParquetDataset with DynEdge.\n",
    "- Adam optimizer\n",
    "- PiecewiseLinearLR (paper schedule: 1e-5 -> 1e-3 -> 1e-5)\n",
    "- EarlyStopping(patience=5) saving best_model.pth + config.yml\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# GraphNeT\n",
    "from graphnet.data.dataset.parquet.parquet_dataset import ParquetDataset\n",
    "from graphnet.data.dataloader import DataLoader\n",
    "\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graphs import KNNGraph\n",
    "from graphnet.models.graphs.nodes import NodesAsPulses\n",
    "\n",
    "from graphnet.models.task import StandardLearnedTask\n",
    "from graphnet.training.loss_functions import LogCoshLoss\n",
    "from graphnet.training.callbacks import ProgressBar, GraphnetEarlyStopping, PiecewiseLinearLR\n",
    "\n",
    "from graphnet.utilities.maths import eps_like\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Task + transforms (SS'deki gibi)\n",
    "# ----------------------------\n",
    "def logarithm(E: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Forward transform: E -> log10(E), safe-clamp to avoid log10(0)/negatives.\"\"\"\n",
    "    E_safe = torch.clamp(E, min=eps_like(E))\n",
    "    return torch.log10(E_safe)\n",
    "\n",
    "\n",
    "def exponential(t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Inverse transform: log10(E) -> E.\"\"\"\n",
    "    return torch.pow(10.0, t)\n",
    "\n",
    "\n",
    "class DepositedEnergyLog10Task(StandardLearnedTask):\n",
    "    default_target_labels = [\"energy\"]\n",
    "    default_prediction_labels = [\"log10_energy_pred\"]\n",
    "    nb_inputs = 1\n",
    "\n",
    "    def _forward(self, x: Tensor) -> Tensor:\n",
    "        # StandardLearnedTask zaten latent'i nb_inputs'e map ediyor.\n",
    "        # Burada sadece 1D output'u döndürmek yeterli.\n",
    "        return x[:, :1]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Build graph_definition + datasets + loaders\n",
    "# ----------------------------\n",
    "def build_datasets_and_loaders(\n",
    "    train_path: str,\n",
    "    val_path: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    ") -> Tuple[object, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        graph_definition, train_loader, val_loader\n",
    "    \"\"\"\n",
    "    # ---- Custom detector import (senin notebooktaki sys.path hack'ini \"script friendly\" yaptım)\n",
    "    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.append(repo_root)\n",
    "\n",
    "    from MyClasses.detector_pone import PONE  # noqa: E402\n",
    "\n",
    "    FEATURES = [\"dom_x\", \"dom_y\", \"dom_z\", \"dom_time\", \"charge\"]  # (x,y,z,t,q)\n",
    "\n",
    "    detector = PONE(\n",
    "        replace_with_identity=[\"charge\", \"dom_time\", \"dom_x\", \"dom_y\", \"dom_z\"]\n",
    "    )\n",
    "\n",
    "    graph_definition = KNNGraph(\n",
    "        detector=detector,\n",
    "        node_definition=NodesAsPulses(),\n",
    "        nb_nearest_neighbours=8,\n",
    "        distance_as_edge_feature=True,\n",
    "    )\n",
    "\n",
    "    # Train/val dataset ayrı\n",
    "    train_ds = ParquetDataset(\n",
    "        path=train_path,\n",
    "        pulsemaps=\"features\",\n",
    "        truth_table=\"truth\",\n",
    "        features=FEATURES,\n",
    "        truth=[\"energy\"],\n",
    "        data_representation=graph_definition,\n",
    "    )\n",
    "\n",
    "    val_ds = ParquetDataset(\n",
    "        path=val_path,\n",
    "        pulsemaps=\"features\",\n",
    "        truth_table=\"truth\",\n",
    "        features=FEATURES,\n",
    "        truth=[\"energy\"],\n",
    "        data_representation=graph_definition,\n",
    "    )\n",
    "\n",
    "    # GraphNeT DataLoader (SS'deki gibi)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        multiprocessing_context=\"spawn\",\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        multiprocessing_context=\"spawn\",\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    return graph_definition, train_loader, val_loader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train\", required=True, help=\"Train merged folder path\")\n",
    "    parser.add_argument(\"--val\", required=True, help=\"Val merged folder path\")\n",
    "    parser.add_argument(\"--outdir\", required=True, help=\"Output directory for logs/models\")\n",
    "\n",
    "    parser.add_argument(\"--epochs\", type=int, default=30)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=4)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    # LR schedule params (paper)\n",
    "    parser.add_argument(\"--lr-max\", type=float, default=1e-3)   # peak\n",
    "    parser.add_argument(\"--lr-min\", type=float, default=1e-5)   # start/end\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "    seed_everything(args.seed, workers=True)\n",
    "\n",
    "    graph_definition, train_loader, val_loader = build_datasets_and_loaders(\n",
    "        train_path=args.train,\n",
    "        val_path=args.val,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Backbone (SS'deki ayarlar)\n",
    "    # ----------------------------\n",
    "    backbone = DynEdge(\n",
    "        nb_inputs=5,                 # FEATURES length\n",
    "        nb_neighbours=8,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        add_global_variables_after_pooling=True,\n",
    "        add_norm_layer=False,\n",
    "        skip_readout=False,\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Task (SS'deki gibi)\n",
    "    # ----------------------------\n",
    "    interval = (1e1, 1e8)\n",
    "    task = DepositedEnergyLog10Task(\n",
    "        hidden_size=backbone.nb_outputs,\n",
    "        loss_function=LogCoshLoss(),\n",
    "        target_labels=[\"energy\"],\n",
    "        prediction_labels=[\"log_energy\"],\n",
    "        transform_prediction_and_target=None,\n",
    "        transform_target=logarithm,\n",
    "        transform_inference=exponential,\n",
    "        transform_support=interval,\n",
    "        loss_weight=None,\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # LR schedule milestones (step-based)\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        steps_per_epoch = len(train_loader)\n",
    "    except TypeError as e:\n",
    "        raise RuntimeError(\n",
    "            \"len(train_loader) bulunamadı. \"\n",
    "            \"Bu durumda steps_per_epoch'i elle hesaplayıp koda koyman gerekir.\"\n",
    "        ) from e\n",
    "\n",
    "    total_steps = steps_per_epoch * args.epochs\n",
    "    warmup_steps = int(0.5 * steps_per_epoch)  # epoch1'in %50'si\n",
    "\n",
    "    # PiecewiseLinearLR: base_lr = lr_max, factors ile lr_min'e in/çık\n",
    "    # lr(t=0)=lr_max*0.01=1e-5, lr(t=warmup)=lr_max*1=1e-3, lr(t=end)=lr_max*0.01=1e-5\n",
    "    factor_min = args.lr_min / args.lr_max  # 1e-5 / 1e-3 = 0.01\n",
    "    scheduler_kwargs = dict(\n",
    "        milestones=[0, warmup_steps, total_steps],\n",
    "        factors=[factor_min, 1.0, factor_min],\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # StandardModel (GraphNeT)\n",
    "    # ----------------------------\n",
    "    model = StandardModel(\n",
    "        graph_definition=graph_definition,\n",
    "        backbone=backbone,\n",
    "        tasks=[task],\n",
    "        optimizer_class=torch.optim.Adam,\n",
    "        optimizer_kwargs={\"lr\": args.lr_max, \"eps\": 1e-3},\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs=scheduler_kwargs,\n",
    "        scheduler_config={\"interval\": \"step\"},\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Callbacks: progress + early stopping(best save/load)\n",
    "    # ----------------------------\n",
    "    early_stopping = GraphnetEarlyStopping(\n",
    "        save_dir=args.outdir,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=5,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    callbacks = [ProgressBar(), early_stopping]\n",
    "\n",
    "    # CSV logger: metrics.csv kaydeder\n",
    "    csv_logger = CSVLogger(save_dir=args.outdir, name=\"pl_logs\")\n",
    "\n",
    "    # Epoch başına 1 log gibi davran (batch spam olmasın)\n",
    "    log_every_n_steps = max(1, steps_per_epoch)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=args.epochs,\n",
    "        logger=csv_logger,\n",
    "        callbacks=callbacks,\n",
    "        log_every_n_steps=log_every_n_steps,\n",
    "        enable_checkpointing=False,  # best'i EarlyStopping callback zaten kaydediyor\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    # Training bitti: EarlyStopping on_fit_end ile \"best_model.pth\" yüklemiş oluyor.\n",
    "    # Ek olarak final çıktıları kaydet:\n",
    "    model.save_config(os.path.join(args.outdir, \"final_config.yml\"))\n",
    "    model.save_state_dict(os.path.join(args.outdir, \"final_state_dict.pth\"))\n",
    "    model.save(os.path.join(args.outdir, \"final_model.pth\"))\n",
    "\n",
    "    print(f\"\\nDONE. Outputs saved under: {args.outdir}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e335518-06f3-4027-8b2e-8ca123debf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_dynedge_energy.py \\\n",
    "  --train /project/def-nahee/kbas/POM_Response_Parquet/train_merged \\\n",
    "  --val   /project/def-nahee/kbas/POM_Response_Parquet/val_merged \\\n",
    "  --outdir /project/def-nahee/kbas/runs/dynedge_energy_run01 \\\n",
    "  --batch-size 4 \\\n",
    "  --num-workers 4 \\\n",
    "  --epochs 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776b183-b2ad-4a22-b114-cd79121a190a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graphnet_env)",
   "language": "python",
   "name": "graphnet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
