{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# P-ONE Muon MC — Reconstruction Performance Plots (TDR-inspired)\n\nThis notebook is a **clean, reproducible** way to make the plots Elisa asked for:\n- **Angular resolution** vs energy (overall + split by declination bands)\n- **Energy resolution** vs energy (68% containment style)\n\n⚠️ **Important limitation (your dataset):** this is **muon-only MC** (Mu± + hadrons), not neutrino MC.\nSo the **neutrino effective area** plots like IceCube-Gen2 **TDR Figure 42** (**A_eff vs Eν**) cannot be reproduced *directly* from this dataset alone.\n\n**TDR mapping (for reference):**\n- Angular resolution (tracks): analogous to **TDR Fig. 43 (Right)**  \n- Energy resolution: analogous to **TDR Table 7** (we turn it into a curve vs energy)\n\n---\n\n## What you need on disk\nThis notebook expects GraphNeT exports:\n- `energy/test_predictions*.csv`\n- `azimuth/test_predictions*.csv`\n- `zenith/test_predictions*.csv`\n\n(Adjust paths in the config section.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 1) Imports + global style\n# =======================\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"once\")\n\n# Global plotting defaults (clean, paper-ish)\nplt.rcParams.update({\n    \"figure.figsize\": (7.2, 4.8),\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 300,\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.25,\n    \"axes.labelsize\": 12,\n    \"axes.titlesize\": 13,\n    \"legend.fontsize\": 10,\n})\n\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2) Configuration\n\nEdit **only this section** when you move folders or want different binning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 2) Configuration\n# =======================\n\n# If you run from .../initiativeX/result_analysis/  -> set BASE to .../initiativeX/\n# You can also hardcode an absolute path.\nBASE = Path.cwd().resolve().parent\n\n# Where to save plots + tables\nOUTDIR = (Path.cwd() / \"plots_performance\").resolve()\nOUTDIR.mkdir(parents=True, exist_ok=True)\n\n# File patterns (GraphNeT)\nENERGY_GLOB  = \"energy/test_predictions*.csv\"\nAZIMUTH_GLOB = \"azimuth/test_predictions*.csv\"\nZENITH_GLOB  = \"zenith/test_predictions*.csv\"\n\n# Energy binning in log10(E/GeV)\nLOGE_BIN_WIDTH = 0.25\nMIN_EVENTS_PER_BIN = 50   # skip bins with too few stats\n\n# TDR-like declination bands shown in IceCube-Gen2 Fig. 42 (Left) legend\n# We'll use the same bins for the angular-resolution split plot.\nSIN_DELTA_BANDS = [\n    ( 0.6,  1.0),\n    ( 0.2,  0.6),\n    (-0.2,  0.2),\n    (-0.6, -0.2),\n    (-1.0, -0.6),\n]\n\nprint(\"BASE  :\", BASE)\nprint(\"OUTDIR:\", OUTDIR)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3) Load GraphNeT `test_predictions*.csv`\n\nWe load the three heads (energy / azimuth / zenith).  \nIf you have **multiple** `test_predictions*.csv` files per folder, this picks the first (sorted). Edit if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 3) Load prediction tables\n# =======================\n\ndef _find_one(base: Path, pattern: str) -> Path:\n    hits = sorted((base).glob(pattern))\n    if len(hits) == 0:\n        raise FileNotFoundError(f\"No files matched: {base / pattern}\")\n    return hits[0]\n\nenergy_path  = _find_one(BASE, ENERGY_GLOB)\nazimuth_path = _find_one(BASE, AZIMUTH_GLOB)\nzenith_path  = _find_one(BASE, ZENITH_GLOB)\n\nenergy_df = pd.read_csv(energy_path)\naz_df     = pd.read_csv(azimuth_path)\nzen_df    = pd.read_csv(zenith_path)\n\nprint(\"ENERGY :\", energy_path,  energy_df.shape)\nprint(\"AZIMUTH:\", azimuth_path, az_df.shape)\nprint(\"ZENITH :\", zenith_path,  zen_df.shape)\n\n# Quick peek\ndisplay(energy_df.head(2))\ndisplay(az_df.head(2))\ndisplay(zen_df.head(2))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4) Merge energy + azimuth + zenith into a single table\n\nWe merge on a common event identifier if it exists (`event_id` is most common).  \nIf no merge key exists, we fall back to row-order concatenation (not recommended).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 4) Merge\n# =======================\n\ndef _choose_merge_key(*dfs: pd.DataFrame) -> str | None:\n    # keys we try, in order\n    candidates = [\"event_id\", \"event_no\", \"event\", \"idx\", \"index\"]\n    common = set(dfs[0].columns)\n    for d in dfs[1:]:\n        common &= set(d.columns)\n    for k in candidates:\n        if k in common:\n            return k\n    return None\n\nmerge_key = _choose_merge_key(energy_df, az_df, zen_df)\nprint(\"Chosen merge key:\", merge_key)\n\n# Suffix all columns from azimuth/zenith (except merge key) to avoid collisions\naz_df2  = az_df.copy()\nzen_df2 = zen_df.copy()\n\nif merge_key is not None:\n    az_df2.columns  = [c if c == merge_key else f\"{c}_AZ\"  for c in az_df2.columns]\n    zen_df2.columns = [c if c == merge_key else f\"{c}_ZEN\" for c in zen_df2.columns]\n\n    df = energy_df.merge(az_df2, on=merge_key, how=\"inner\")\n    df = df.merge(zen_df2, on=merge_key, how=\"inner\")\nelse:\n    # Fallback: assume identical row order\n    df = pd.concat(\n        [energy_df, az_df.add_suffix(\"_AZ\"), zen_df.add_suffix(\"_ZEN\")],\n        axis=1,\n    )\n\nprint(\"Merged df:\", df.shape)\ndisplay(df.head(3))\n\n# Basic sanity checks\nif merge_key is not None:\n    dup = df[merge_key].duplicated().sum()\n    if dup > 0:\n        print(f\"WARNING: {dup} duplicated {merge_key} values after merge.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5) Derived quantities (angles, energy, sinδ, opening angle)\n\nWe compute:\n- `E_true_GeV`, `E_pred_GeV`\n- `true_zenith_rad`, `pred_zenith_rad`, `true_azimuth_rad`, `pred_azimuth_rad`\n- `sin_delta ≈ -cos(true_zenith_rad)` (South Pole convention approximation)\n- `opening_angle_deg` (Δψ)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 5) Helpers\n# =======================\n\ndef _first_existing(df: pd.DataFrame, candidates: list[str]) -> str:\n    for c in candidates:\n        if c in df.columns:\n            return c\n    raise KeyError(f\"None of these columns exist: {candidates}\")\n\ndef _to_radians(arr: np.ndarray) -> np.ndarray:\n    # Heuristic: if values look like degrees, convert.\n    # zenith: [0, π] rad vs [0, 180] deg\n    # azimuth: [0, 2π] rad vs [0, 360] deg\n    m = np.nanmax(arr)\n    if m > 2*np.pi + 0.2:\n        return np.deg2rad(arr)\n    return arr\n\ndef _wrap_0_2pi(phi: np.ndarray) -> np.ndarray:\n    return np.mod(phi, 2*np.pi)\n\ndef compute_opening_angle_deg(th_true, ph_true, th_pred, ph_pred) -> np.ndarray:\n    # u = (sinθ cosφ, sinθ sinφ, cosθ)\n    ux_t = np.sin(th_true) * np.cos(ph_true)\n    uy_t = np.sin(th_true) * np.sin(ph_true)\n    uz_t = np.cos(th_true)\n\n    ux_p = np.sin(th_pred) * np.cos(ph_pred)\n    uy_p = np.sin(th_pred) * np.sin(ph_pred)\n    uz_p = np.cos(th_pred)\n\n    dot = ux_t*ux_p + uy_t*uy_p + uz_t*uz_p\n    dot = np.clip(dot, -1.0, 1.0)\n    return np.degrees(np.arccos(dot))\n\ndef build_logE_bins(logE: pd.Series, width: float) -> np.ndarray:\n    xmin = np.floor(np.nanmin(logE) / width) * width\n    xmax = np.ceil(np.nanmax(logE) / width) * width\n    return np.arange(xmin, xmax + width, width)\n\ndef binned_quantiles(values: pd.Series, bins: pd.Categorical, qs=(0.16, 0.50, 0.84)) -> pd.DataFrame:\n    g = values.groupby(bins, observed=True)\n    out = g.quantile(list(qs)).unstack()\n    out.columns = [f\"q{int(q*100):02d}\" for q in qs]\n    out[\"n\"] = g.size()\n    out[\"logE_center\"] = [iv.mid for iv in out.index]\n    return out.reset_index(drop=True)\n\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 5) Resolve columns + compute derived columns\n# =======================\n\n# ---- Energy truth/pred ----\n# Prefer linear energy columns if they exist, else reconstruct from log10 columns.\nTRUE_E_COL_CANDIDATES = [\"true_energy\", \"energy_true\", \"E_true\", \"true_E\"]\nPRED_E_COL_CANDIDATES = [\"pred_energy\", \"energy_pred\", \"E_pred\", \"pred_E\"]\n\nTRUE_LOGE_COL_CANDIDATES = [\"true_log10_energy\", \"log10_true_energy\", \"true_logE\"]\nPRED_LOGE_COL_CANDIDATES = [\"pred_log10_energy\", \"log10_pred_energy\", \"pred_logE\"]\n\nif any(c in df.columns for c in TRUE_E_COL_CANDIDATES):\n    true_E_col = _first_existing(df, TRUE_E_COL_CANDIDATES)\n    E_true_GeV = df[true_E_col].to_numpy(dtype=float)\nelif any(c in df.columns for c in TRUE_LOGE_COL_CANDIDATES):\n    true_logE_col = _first_existing(df, TRUE_LOGE_COL_CANDIDATES)\n    E_true_GeV = 10 ** df[true_logE_col].to_numpy(dtype=float)\nelse:\n    raise KeyError(\"Could not find true energy column (linear or log10).\")\n\nif any(c in df.columns for c in PRED_E_COL_CANDIDATES):\n    pred_E_col = _first_existing(df, PRED_E_COL_CANDIDATES)\n    E_pred_GeV = df[pred_E_col].to_numpy(dtype=float)\nelif any(c in df.columns for c in PRED_LOGE_COL_CANDIDATES):\n    pred_logE_col = _first_existing(df, PRED_LOGE_COL_CANDIDATES)\n    E_pred_GeV = 10 ** df[pred_logE_col].to_numpy(dtype=float)\nelse:\n    raise KeyError(\"Could not find predicted energy column (linear or log10).\")\n\ndf[\"E_true_GeV\"] = E_true_GeV\ndf[\"E_pred_GeV\"] = E_pred_GeV\ndf[\"logE_true\"]  = np.log10(df[\"E_true_GeV\"].clip(min=1e-9))\n\n# ---- Angles ----\n# The current notebook uses these (GraphNeT-style):\n# true_zenith_radian_ZEN, pred_zenith_radian_ZEN, true_azimuth_radian_AZ, pred_azimuth_radian_AZ\nTRUE_TH_CANDS = [\"true_zenith_radian_ZEN\", \"true_zenith_radian\", \"true_zenith_ZEN\", \"true_zenith\"]\nPRED_TH_CANDS = [\"pred_zenith_radian_ZEN\", \"pred_zenith_radian\", \"pred_zenith_ZEN\", \"pred_zenith\"]\n\nTRUE_PH_CANDS = [\"true_azimuth_radian_AZ\", \"true_azimuth_radian\", \"true_azimuth_AZ\", \"true_azimuth\"]\nPRED_PH_CANDS = [\"pred_azimuth_radian_AZ\", \"pred_azimuth_radian\", \"pred_azimuth_AZ\", \"pred_azimuth\"]\n\ntrue_th = df[_first_existing(df, TRUE_TH_CANDS)].to_numpy(dtype=float)\npred_th = df[_first_existing(df, PRED_TH_CANDS)].to_numpy(dtype=float)\ntrue_ph = df[_first_existing(df, TRUE_PH_CANDS)].to_numpy(dtype=float)\npred_ph = df[_first_existing(df, PRED_PH_CANDS)].to_numpy(dtype=float)\n\ntrue_th = _to_radians(true_th)\npred_th = _to_radians(pred_th)\ntrue_ph = _wrap_0_2pi(_to_radians(true_ph))\npred_ph = _wrap_0_2pi(_to_radians(pred_ph))\n\ndf[\"true_zenith_rad\"]  = true_th\ndf[\"pred_zenith_rad\"]  = pred_th\ndf[\"true_azimuth_rad\"] = true_ph\ndf[\"pred_azimuth_rad\"] = pred_ph\n\n# South Pole approximation: sinδ ≈ -cos(zenith)\ndf[\"sin_delta\"] = -np.cos(df[\"true_zenith_rad\"])\n\n# Opening angle Δψ\ndf[\"opening_angle_deg\"] = compute_opening_angle_deg(\n    df[\"true_zenith_rad\"].to_numpy(),\n    df[\"true_azimuth_rad\"].to_numpy(),\n    df[\"pred_zenith_rad\"].to_numpy(),\n    df[\"pred_azimuth_rad\"].to_numpy(),\n)\n\nprint(\"opening_angle_deg summary:\")\ndisplay(df[\"opening_angle_deg\"].describe(percentiles=[0.5, 0.68, 0.9, 0.95, 0.99]))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6) Plot — Angular resolution (overall): median Δψ vs energy\n\n**x-axis:** True energy \\(E\\) [GeV] (log scale)  \n**y-axis:** Median opening angle \\(\\Psi_{\\rm med}\\) [deg]  \nShaded band: 16–84% (≈ 68% containment)\n\nTDR analogue: **Fig. 43 (Right)** (track angular resolution).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 6) Angular resolution (overall)\n# =======================\n\nbins = build_logE_bins(df[\"logE_true\"], LOGE_BIN_WIDTH)\ndf[\"logE_bin\"] = pd.cut(df[\"logE_true\"], bins=bins, include_lowest=True)\n\nq = binned_quantiles(df[\"opening_angle_deg\"], df[\"logE_bin\"], qs=(0.16, 0.50, 0.84))\nq = q[q[\"n\"] >= MIN_EVENTS_PER_BIN].copy()\n\nE_centers = 10 ** q[\"logE_center\"].to_numpy()\n\nfig, ax = plt.subplots()\nax.plot(E_centers, q[\"q50\"], marker=\"o\", linestyle=\"-\", label=r\"median $\\Psi$\")\nax.fill_between(E_centers, q[\"q16\"], q[\"q84\"], alpha=0.2, label=\"16–84%\")\n\nax.set_xscale(\"log\")\nax.set_xlabel(r\"True energy $E$ [GeV]\")\nax.set_ylabel(r\"Opening angle $\\Psi$ [deg]\")\nax.set_title(\"Angular resolution (overall)\")\nax.legend()\n\nout_png = OUTDIR / \"angular_resolution_overall.png\"\nout_pdf = OUTDIR / \"angular_resolution_overall.pdf\"\nfig.savefig(out_png, bbox_inches=\"tight\")\nfig.savefig(out_pdf, bbox_inches=\"tight\")\nplt.show()\n\nprint(\"Saved:\", out_png)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7) Plot — Angular resolution split by declination bands (sinδ)\n\n**x-axis:** True energy \\(E\\) [GeV] (log scale)  \n**y-axis:** Median opening angle \\(\\Psi_{\\rm med}\\) [deg]  \nCurves: TDR-like **sinδ** bands.\n\nTDR analogue: again **Fig. 43 (Right)**, but shown per band.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 7) Angular resolution split by sinδ\n# =======================\n\nfig, ax = plt.subplots()\n\nfor lo, hi in SIN_DELTA_BANDS:\n    sub = df[(df[\"sin_delta\"] > lo) & (df[\"sin_delta\"] <= hi)].copy()\n    if len(sub) == 0:\n        print(f\"Band sinδ in ({lo},{hi}] has 0 events -> skipped.\")\n        continue\n\n    sub[\"logE_bin\"] = pd.cut(sub[\"logE_true\"], bins=bins, include_lowest=True)\n    med = sub.groupby(\"logE_bin\", observed=True)[\"opening_angle_deg\"].median()\n    counts = sub.groupby(\"logE_bin\", observed=True)[\"opening_angle_deg\"].size()\n\n    # keep only bins with enough stats\n    med = med[counts >= MIN_EVENTS_PER_BIN].dropna()\n    if len(med) == 0:\n        print(f\"Band sinδ in ({lo},{hi}] has no bins with n >= {MIN_EVENTS_PER_BIN}.\")\n        continue\n\n    logE_center = np.array([iv.mid for iv in med.index])\n    E_center = 10 ** logE_center\n    ax.plot(E_center, med.values, marker=\"o\", linestyle=\"-\", label=f\"({lo}, {hi}]\")\n\nax.set_xscale(\"log\")\nax.set_xlabel(r\"True energy $E$ [GeV]\")\nax.set_ylabel(r\"$\\Psi_{\\rm med}$ [deg]\")\nax.set_title(r\"Angular resolution by declination band (sin$\\delta$)\")\nax.legend(title=r\"sin$\\delta$ bin\")\n\nout_png = OUTDIR / \"angular_resolution_by_sindelta.png\"\nout_pdf = OUTDIR / \"angular_resolution_by_sindelta.pdf\"\nfig.savefig(out_png, bbox_inches=\"tight\")\nfig.savefig(out_pdf, bbox_inches=\"tight\")\nplt.show()\n\nprint(\"Saved:\", out_png)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8) Plot — Energy resolution vs energy (68% containment style)\n\nFor each energy bin we compute:\n- **bias:** median of \\(\\log_{10}(E_{\\rm pred}/E_{\\rm true})\\)\n- **resolution (68%):** half-width of the 16–84% interval of \\(\\log_{10}(E_{\\rm pred}/E_{\\rm true})\\)\n\nThis is the closest “curve version” of **TDR Table 7**.\n\n**x-axis:** True energy \\(E\\) [GeV] (log scale)  \n**y-axis:** \\(\\sigma_{68}\\big[\\log_{10}(E_{\\rm pred}/E_{\\rm true})\\big]\\)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 8) Energy resolution\n# =======================\n\nratio = (df[\"E_pred_GeV\"] / df[\"E_true_GeV\"]).to_numpy(dtype=float)\nratio = np.where(np.isfinite(ratio) & (ratio > 0), ratio, np.nan)\n\ndf[\"log10_ratio\"] = np.log10(ratio)\n\n# Bin in true logE\ndf[\"logE_bin\"] = pd.cut(df[\"logE_true\"], bins=bins, include_lowest=True)\n\ng = df[\"log10_ratio\"].groupby(df[\"logE_bin\"], observed=True)\nq16 = g.quantile(0.16)\nq50 = g.quantile(0.50)\nq84 = g.quantile(0.84)\nn   = g.size()\n\nsummary = pd.DataFrame({\n    \"logE_center\": [iv.mid for iv in q50.index],\n    \"E_center_GeV\": 10 ** np.array([iv.mid for iv in q50.index]),\n    \"n\": n.to_numpy(),\n    \"q16\": q16.to_numpy(),\n    \"q50\": q50.to_numpy(),\n    \"q84\": q84.to_numpy(),\n})\nsummary = summary[summary[\"n\"] >= MIN_EVENTS_PER_BIN].dropna().reset_index(drop=True)\n\n# 68% half-width in log10 space\nsummary[\"sigma68_log10\"] = 0.5 * (summary[\"q84\"] - summary[\"q16\"])\nsummary[\"bias_log10\"]    = summary[\"q50\"]\n\n# Optional: convert sigma68_log10 to an approximate fractional resolution\n# If sigma is in log10, a multiplicative 1-sigma factor is 10^sigma.\nsummary[\"frac68_approx\"] = (10 ** summary[\"sigma68_log10\"]) - 1.0\n\ndisplay(summary.head(10))\n\n# ---- Plot: sigma68(log10 ratio) vs energy ----\nfig, ax = plt.subplots()\nax.plot(summary[\"E_center_GeV\"], summary[\"sigma68_log10\"], marker=\"o\", linestyle=\"-\")\nax.set_xscale(\"log\")\nax.set_xlabel(r\"True energy $E$ [GeV]\")\nax.set_ylabel(r\"$\\sigma_{68}[\\log_{10}(E_{pred}/E_{true})]$\")\nax.set_title(\"Energy resolution (68% containment, log10 space)\")\n\nout_png = OUTDIR / \"energy_resolution_sigma68_log10.png\"\nout_pdf = OUTDIR / \"energy_resolution_sigma68_log10.pdf\"\nfig.savefig(out_png, bbox_inches=\"tight\")\nfig.savefig(out_pdf, bbox_inches=\"tight\")\nplt.show()\n\nprint(\"Saved:\", out_png)\n\n# ---- Plot: bias vs energy ----\nfig, ax = plt.subplots()\nax.plot(summary[\"E_center_GeV\"], summary[\"bias_log10\"], marker=\"o\", linestyle=\"-\")\nax.set_xscale(\"log\")\nax.set_xlabel(r\"True energy $E$ [GeV]\")\nax.set_ylabel(r\"median $\\log_{10}(E_{pred}/E_{true})$\")\nax.set_title(\"Energy bias (median log10 ratio)\")\n\nout_png2 = OUTDIR / \"energy_bias_median_log10.png\"\nout_pdf2 = OUTDIR / \"energy_bias_median_log10.pdf\"\nfig.savefig(out_png2, bbox_inches=\"tight\")\nfig.savefig(out_pdf2, bbox_inches=\"tight\")\nplt.show()\n\nprint(\"Saved:\", out_png2)\n\n# Save table\nout_csv = OUTDIR / \"binned_energy_resolution.csv\"\nsummary.to_csv(out_csv, index=False)\nprint(\"Saved:\", out_csv)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9) Effective area (A_eff) — **not directly possible with this muon-only dataset**\n\nElisa’nın dediği “effective areas” (TDR Fig. 42) normalde **neutrino MC** ile yapılır.\n\n### Neden bu dataset ile olmuyor?\nSenin sample:\n- **Mu± + hadrons** (charged lepton injection), **neutrino injection değil**\n- Effective area için tipik olarak gerekenler:\n  1) **Thrown neutrino spectrum + weights** (I3MCWeightDict / LeptonWeighter weights)\n  2) **Thrown area / solid angle definition** (generation surface + angular phase space)\n  3) **Selection definition** (detector-level + quality cuts)\n\nBu notebook şu an yalnızca **reco performance** (angular + energy resolution) yapıyor.\n\n### Eğer daha sonra neutrino MC gelir ise:\nBu section’a A_eff hesaplamayı eklemek kolay:  \n`A_eff(E) = (sum weights of selected events in bin) / (sum weights of thrown events in bin) * A_thrown`\n\nAşağıdaki cell sadece “iskelet” – IceTray yoksa çalışmaz, normal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =======================\n# 9) OPTIONAL scaffold: inspect weights in I3 (if you have IceTray locally)\n# =======================\n\n# This cell is OPTIONAL.\n# It helps you check which weight keys exist in your i3 files.\n# If IceTray is not available on your machine, it will simply print a message.\n\ntry:\n    from icecube import dataio, icetray, dataclasses, simclasses\nexcept Exception as e:\n    print(\"IceTray not available here -> skipping I3 weight inspection.\")\n    print(\"Error:\", repr(e))\nelse:\n    # EDIT THIS to point to one example i3 file on your system\n    example_i3 = None  # e.g. Path(\"/path/to/cls_4910.i3\")\n\n    if example_i3 is None:\n        print(\"Set `example_i3` to an existing .i3/.i3.gz file to inspect weight keys.\")\n    else:\n        f = dataio.I3File(str(example_i3), \"r\")\n        fr = f.pop_frame()\n        print(\"Frame keys:\", list(fr.keys()))\n\n        # Common places to look for weights:\n        for k in [\"I3MCWeightDict\", \"EventWeight\", \"EventWeights\", \"LeptonInjectorWeight\"]:\n            if k in fr:\n                obj = fr[k]\n                print(f\"Found {k}: type={type(obj)}\")\n                try:\n                    print(dict(obj))\n                except Exception:\n                    print(obj)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10) Output checklist\n\nAfter running, you should have these files under `plots_performance/`:\n- `angular_resolution_overall.(png/pdf)`\n- `angular_resolution_by_sindelta.(png/pdf)`\n- `energy_resolution_sigma68_log10.(png/pdf)`\n- `energy_bias_median_log10.(png/pdf)`\n- `binned_energy_resolution.csv`\n\nIf a plot looks noisy:\n- increase `MIN_EVENTS_PER_BIN`\n- increase `LOGE_BIN_WIDTH` (e.g. 0.3)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "graphnet_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}