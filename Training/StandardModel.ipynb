{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61470d3-8ad1-4298-9f0e-db406503fe69",
   "metadata": {},
   "source": [
    "# GraphNeT Training Script (P-ONE, OM-center geometry) â€” Notes & Rationale\n",
    "\n",
    "I wrote this document to explain my training script end-to-end: what each part does, why I chose this graph representation, what SLURM resources I request, and what the â€œmissing GCD / missing PMT positionsâ€ problem changes in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) What I am trying to do\n",
    "\n",
    "I want to train a Graph Neural Network (GraphNeT) to predict **event energy** (`truth[\"energy\"]`) using detector pulse information stored in **Parquet**.\n",
    "\n",
    "My dataset limitation is important:\n",
    "\n",
    "- I do **not** have per-PMT positions.\n",
    "- I only have **Optical Module (OM) center** positions (`dom_x, dom_y, dom_z`).\n",
    "- Pulses have at least: OM-center position, time, charge.\n",
    "\n",
    "So I need a graph design that uses geometry but does not create â€œfake localityâ€ inside a single OM.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) High-level pipeline (what happens when I run the script)\n",
    "\n",
    "When I execute the script, this is the flow:\n",
    "\n",
    "1. I define a **Detector mapping** (`PONE`) so GraphNeT knows how to interpret input columns.\n",
    "2. I define a **graph representation** (`KNNGraph`) with OM-level nodes using `ClusterSummaryFeatures`.\n",
    "3. I create a **ParquetDataset**, which loads events and builds one graph per event.\n",
    "4. I create a **DataLoader**, which batches graphs for training.\n",
    "5. I define a **GNN backbone** (`DynEdge`).\n",
    "6. I define a **task head** (`EnergyReconstruction`) to regress energy.\n",
    "7. I create a **StandardModel** from (data_representation + backbone + task).\n",
    "8. I train with `model.fit(...)` on GPU if available.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) The GCD / geometry problem in my dataset\n",
    "\n",
    "### 2.1 What I am missing\n",
    "Normally, a â€œgoodâ€ detector geometry setup (GCD-like information) would let me place each PMT precisely inside an OM. But my dataset does not have that. Instead:\n",
    "\n",
    "- All pulses from the same OM share the exact same `(dom_x, dom_y, dom_z)` (the OM center).\n",
    "\n",
    "### 2.2 Why pulse-level KNN is a problem here\n",
    "If I used:\n",
    "\n",
    "- `NodesAsPulses()` (node = pulse)\n",
    "- KNN edges based on `(dom_x, dom_y, dom_z)`\n",
    "\n",
    "then many pulses would be at identical coordinates (same OM center). That creates a failure mode:\n",
    "\n",
    "- For a given pulse-node, the â€œclosest K pulsesâ€ are often just other pulses from the **same OM** (distance 0).\n",
    "- So the graph ends up dominated by **within-OM** edges, not **between-OM** edges.\n",
    "- That reduces real spatial information flow across the detector and can confuse the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) My solution: OM-level nodes via clustering + KNN between OMs\n",
    "\n",
    "To avoid the â€œsame OM pulse cliqueâ€ problem, I switched node construction to:\n",
    "\n",
    "- `ClusterSummaryFeatures(cluster_on=[\"dom_x\", \"dom_y\", \"dom_z\"], ...)`\n",
    "\n",
    "This makes:\n",
    "\n",
    "- **One node per OM** (per event), created by clustering pulses that share the same OM-center position.\n",
    "- Node features become **summary statistics** of the pulses in that OM.\n",
    "\n",
    "Then I connect these OM nodes using:\n",
    "\n",
    "- `KNNGraph(nb_nearest_neighbours=K, columns=[0,1,2])`\n",
    "\n",
    "Meaning:\n",
    "\n",
    "- For each OM node, I connect it to its **K nearest neighboring OMs** in 3D geometry.\n",
    "\n",
    "This fits my data limitation: I cannot resolve PMT micro-geometry, but I can still use OM spatial layout.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) What `ClusterSummaryFeatures` features mean (the ones I see in config)\n",
    "\n",
    "My `ClusterSummaryFeatures` configuration builds OM-level node features. Here is what each one means conceptually.\n",
    "\n",
    "### 4.1 Geometry identity\n",
    "Because I cluster on `dom_x, dom_y, dom_z`, every node starts with these as features:\n",
    "- `dom_x, dom_y, dom_z`\n",
    "\n",
    "These are also used for KNN distance (I set `columns=[0,1,2]`).\n",
    "\n",
    "### 4.2 Charge features\n",
    "#### `total_charge`\n",
    "- Sum of all pulse charges in that OM (a proxy for â€œhow brightâ€ that OM was in the event).\n",
    "- With `charge_standardization=\"log\"`, the feature is log10-scaled internally.\n",
    "\n",
    "#### `charge_after_t` (e.g., `charge_after_10ns`, `charge_after_50ns`, `charge_after_100ns`)\n",
    "- â€œHow much charge has accumulated within **t nanoseconds** after the reference time.â€\n",
    "- These capture the *early vs late* charge buildup shape.\n",
    "- Also log10-scaled internally due to `charge_standardization=\"log\"`.\n",
    "\n",
    "### 4.3 Time features\n",
    "There is a reference time used internally (charge-weighted median time in the cluster), and some time features are expressed relative to it.\n",
    "\n",
    "#### `time_of_first_hit`\n",
    "- Time of the first pulse in that OM (then shifted relative to the internal reference time).\n",
    "- Standardized with `time_standardization=0.001` (a multiplicative scaling).\n",
    "\n",
    "#### `time_spread`\n",
    "- â€œHow wide is the pulse time distributionâ€ inside the OM.\n",
    "- Roughly (last time âˆ’ first time), depending on implementation.\n",
    "- Scaled by `0.001`.\n",
    "\n",
    "#### `time_std`\n",
    "- Standard deviation of pulse times inside the OM.\n",
    "- Scaled by `0.001`.\n",
    "\n",
    "#### `time_after_charge_pctX` (e.g., `time_after_charge_pct50`)\n",
    "- For a given percentile X, it measures:  \n",
    "  â€œHow long until the OM accumulated **X% of its total charge**?â€\n",
    "- This describes the temporal charge collection profile.\n",
    "- Scaled by `0.001`.\n",
    "\n",
    "### 4.4 Counts\n",
    "#### `add_counts=True` â†’ `counts`\n",
    "- Adds the number of pulses in that OM cluster (pulse multiplicity).\n",
    "- This helps the model distinguish â€œfew big pulsesâ€ vs â€œmany small pulses,â€ etc.\n",
    "\n",
    "### 4.5 Important standardization note (to avoid double transforms)\n",
    "Because `ClusterSummaryFeatures` already standardizes:\n",
    "- charge features (log10)\n",
    "- time features (multiply by 0.001)\n",
    "\n",
    "I keep my `PONE._charge()` and `PONE._time()` as identity (no extra log or scaling).  \n",
    "That avoids double-standardizing the same information.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Graph connectivity: what KNN means in my script\n",
    "\n",
    "I set:\n",
    "- `K = 8`\n",
    "- `columns = [0, 1, 2]`\n",
    "\n",
    "Because the node feature vector begins with:\n",
    "- `[dom_x, dom_y, dom_z, ...]`\n",
    "\n",
    "So KNN distance uses exactly those geometry columns.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Each OM node connects to its **8 nearest OM nodes** in 3D space.\n",
    "- This defines message-passing neighborhood size.\n",
    "- If I see memory issues or slow graph building, I can reduce K (e.g., 8 â†’ 6 or 4).\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Model architecture: DynEdge + EnergyReconstruction\n",
    "\n",
    "### 6.1 Backbone: `DynEdge`\n",
    "I use `DynEdge` as the GNN backbone. It performs message passing on the graph and produces an event-level representation.\n",
    "\n",
    "I set:\n",
    "- `nb_inputs = data_representation.nb_outputs`  \n",
    "  So the backbone input dimension matches the OM-node feature dimension produced by `ClusterSummaryFeatures`.\n",
    "\n",
    "I also set:\n",
    "- `global_pooling_schemes=[\"min\", \"max\", \"mean\"]`  \n",
    "  This means the model forms event-level features by pooling node embeddings using multiple aggregation operators.\n",
    "\n",
    "### 6.2 Task head: `EnergyReconstruction`\n",
    "I use:\n",
    "- `EnergyReconstruction(target_labels=[\"energy\"])`\n",
    "- `LogCoshLoss()`\n",
    "\n",
    "So the model is trained as a regression model predicting event energy.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Training setup in my script\n",
    "\n",
    "I train with `StandardModel.fit(...)` using a **single GPU** (if CUDA is available).\n",
    "\n",
    "My key training choices:\n",
    "\n",
    "- **Epochs**\n",
    "  - `MAX_EPOCHS = 5`\n",
    "  - This is a short â€œbaseline runâ€ to confirm the full pipeline works end-to-end.\n",
    "\n",
    "- **GPU selection**\n",
    "  - `GPUS = 1 if torch.cuda.is_available() else None`\n",
    "  - If CUDA is available, training runs on 1 GPU (in my case, a MIG slice).\n",
    "\n",
    "- **Distributed strategy**\n",
    "  - `distribution_strategy=\"auto\"`\n",
    "  - This prevents PyTorch Lightning from trying to use notebook-incompatible DDP settings.\n",
    "  - Since I only have 1 GPU, I do **not** need DDP anyway.\n",
    "\n",
    "- **H100 performance tweak**\n",
    "  - I set:\n",
    "    ```python\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    ```\n",
    "  - This can improve performance on H100 Tensor Cores (without changing my model code).\n",
    "\n",
    "---\n",
    "\n",
    "## 8) SLURM resources: what I request and why\n",
    "\n",
    "Even though the model trains on **GPU**, I still need **CPU** for:\n",
    "- reading Parquet from disk,\n",
    "- clustering pulses into OM nodes (`ClusterSummaryFeatures`),\n",
    "- constructing KNN edges between OM nodes,\n",
    "- batching in the DataLoader.\n",
    "\n",
    "If I request too little CPU, the GPU will often **wait** for the next batch.\n",
    "\n",
    "### 8.1 The `salloc` configuration I plan to use\n",
    "\n",
    "I will allocate:\n",
    "\n",
    "```bash\n",
    "salloc --time=2:00:00 \\\n",
    "  --account=def-nahee \\\n",
    "  --mem=32G \\\n",
    "  --cpus-per-task=4 \\\n",
    "  --gpus-per-node=nvidia_h100_80gb_hbm3_1g.10gb:1\n",
    "\n",
    "\n",
    "Here is what each flag means:\n",
    "\n",
    "- `--time=2:00:00`  \n",
    "  A 2-hour walltime limit for the job.\n",
    "\n",
    "- `--account=def-nahee`  \n",
    "  The SLURM account that gets charged for the allocation.\n",
    "\n",
    "- `--mem=32G`  \n",
    "  32 GB of CPU RAM for Parquet reading, OM clustering, graph construction, and batching.\n",
    "\n",
    "- `--cpus-per-task=4`  \n",
    "  4 CPU cores for the process. I need CPU for the DataLoader + graph building even when training on GPU.  \n",
    "  This matches a stable baseline like `NUM_WORKERS=2`.\n",
    "\n",
    "- `--gpus-per-node=nvidia_h100_80gb_hbm3_1g.10gb:1`  \n",
    "  1 GPU allocation on the node, specifically an H100 MIG slice (1g.10gb), i.e. about ~10 GB VRAM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e3fcb-2d36-4a50-96aa-7cb0f7cccd1e",
   "metadata": {},
   "source": [
    "\n",
    "### 8.2 How I run the script after `salloc`\n",
    "\n",
    "After SLURM grants the allocation, I run the script inside the allocated resources using `srun`:\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "srun --cpus-per-task=4 python my_training_script.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095a87f-415a-4791-a7b1-9dc059a0a19a",
   "metadata": {},
   "source": [
    "I use `srun` because it launches the process through SLURM, so resource binding is correct and PyTorch Lightning detects the SLURM environment cleanly.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Practical tuning knobs (if it is slow or if I hit OOM)\n",
    "\n",
    "### 9.1 If the run feels slow (especially at the start)\n",
    "\n",
    "It is normal if the progress bar stays at `0/...` for a while at the beginning, because:\n",
    "\n",
    "- DataLoader worker processes are being created\n",
    "- the filesystem cache is warming up\n",
    "- the first event graph is being constructed (clustering + KNN edges)\n",
    "\n",
    "What I adjust first:\n",
    "\n",
    "- With `--cpus-per-task=4`, I keep `NUM_WORKERS=2` (stable baseline).\n",
    "- If I request `--cpus-per-task=8` later, I try `NUM_WORKERS=4`.\n",
    "\n",
    "If multiprocessing causes issues, I temporarily switch to:\n",
    "\n",
    "- `NUM_WORKERS=0` (debug mode; often more stable, sometimes slower)\n",
    "\n",
    "### 9.2 If I get CUDA out-of-memory (MIG 10GB)\n",
    "\n",
    "With a 10GB MIG slice, out-of-memory is the most likely GPU-side failure mode. I change settings in this order:\n",
    "\n",
    "1. Reduce batch size: `BATCH_SIZE = 4 -> 2`\n",
    "2. Reduce edges: `K = 8 -> 6 -> 4`\n",
    "3. Simplify `ClusterSummaryFeatures` (fewer percentiles, fewer `charge_after_t` times, disable optional time features)\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Why this setup is a good baseline for my dataset\n",
    "\n",
    "Because I only have OM-center geometry (no PMT positions):\n",
    "\n",
    "- Pulse-level KNN can become degenerate because many pulses share identical `(dom_x, dom_y, dom_z)` and connect mostly within the same OM.\n",
    "- OM-level clustering (`ClusterSummaryFeatures`) creates one physically meaningful node per OM.\n",
    "- KNN edges then connect nearby OMs, which is the best spatial structure I can trust with my available geometry.\n",
    "- DynEdge is a strong baseline GNN backbone for detector-style graphs in GraphNeT.\n",
    "\n",
    "Overall, this pipeline is stable to debug, physically reasonable under my constraints, and realistic to run on a 10GB MIG slice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d141d7aa-5c0a-49ce-bb76-f03c5101bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[33mWARNING \u001b[0m 2026-01-05 06:11:02 - has_jammy_flows_package - `jammy_flows` not available. Normalizing Flow functionality is missing.\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[33mWARNING \u001b[0m 2026-01-05 06:11:02 - has_icecube_package - `icecube` not available. Some functionality may be missing.\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[33mWARNING \u001b[0m 2026-01-05 06:11:03 - has_km3net_package - `km3net` not available. Some functionality may be missing.\u001b[0m\n",
      "[2026-01-05 06:11:03] ==> Starting section: 0) Imports and Helper\n",
      "[2026-01-05 06:11:03] ==> Starting section: 0) Global Settings\n",
      "[2026-01-05 06:11:03] ==> Starting section: 0) Environment Information\n",
      "Device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3 MIG 1g.10gb\n",
      "Set torch matmul precision to: medium\n",
      "os.cpu_count: 48\n",
      "affinity cores: 1\n",
      "affinity set (first 20): [0]\n",
      "SLURM_CPUS_PER_TASK: None\n",
      "SLURM_JOB_CPUS_PER_NODE: 1\n",
      "[2026-01-05 06:11:03] ==> Starting section: 0) My Classes: PONE (Detector)\n"
     ]
    }
   ],
   "source": [
    "## 0) Imports and Little Helper Function:\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, Callable\n",
    "\n",
    "import torch\n",
    "from graphnet.models.detector.detector import Detector\n",
    "\n",
    "from graphnet.models.data_representation import KNNGraph\n",
    "from graphnet.models.data_representation.graphs import ClusterSummaryFeatures\n",
    "\n",
    "from graphnet.models.gnn.dynedge import DynEdge\n",
    "from graphnet.data.dataset.parquet.parquet_dataset import ParquetDataset\n",
    "from graphnet.data.dataloader import DataLoader\n",
    "from graphnet.models.task.reconstruction import EnergyReconstruction\n",
    "from graphnet.training.loss_functions import LogCoshLoss\n",
    "from graphnet.models import StandardModel\n",
    "\n",
    "\n",
    "def section_banner(name: str) -> None:\n",
    "    print(\n",
    "        f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ==> Starting section: {name}\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "\n",
    "section_banner(\"0) Imports and Helper\")\n",
    "\n",
    "\n",
    "## 0) Global Settings:\n",
    "\n",
    "section_banner(\"0) Global Settings\")\n",
    "\n",
    "PARQUET_ROOT = \"/project/def-nahee/kbas/POM_Response_Parquet/merged\"\n",
    "PULSEMAPS = \"features\"   # pulse/hit parquet folder\n",
    "TRUTH_TABLE = \"truth\"    # truth parquet folder\n",
    "\n",
    "TRUTH_LABELS = [\"energy\"]\n",
    "\n",
    "\n",
    "## 0) Environment Information:\n",
    "\n",
    "section_banner(\"0) Environment Information\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    print(\"Set torch matmul precision to: medium\")\n",
    "else:\n",
    "    print(\"GPU: (not available)\")\n",
    "\n",
    "print(\"os.cpu_count:\", os.cpu_count())\n",
    "\n",
    "if hasattr(os, \"sched_getaffinity\"):\n",
    "    affinity = sorted(os.sched_getaffinity(0))\n",
    "    print(\"affinity cores:\", len(affinity))\n",
    "    print(\"affinity set (first 20):\", affinity[:20])\n",
    "\n",
    "print(\"SLURM_CPUS_PER_TASK:\", os.environ.get(\"SLURM_CPUS_PER_TASK\"))\n",
    "print(\"SLURM_JOB_CPUS_PER_NODE:\", os.environ.get(\"SLURM_JOB_CPUS_PER_NODE\"))\n",
    "\n",
    "\n",
    "\n",
    "## 0) My Classes: PONE (Detector)\n",
    "\n",
    "section_banner(\"0) My Classes: PONE (Detector)\")\n",
    "\n",
    "\n",
    "class PONE(Detector):\n",
    "    \"\"\"Detector class for P-ONE.\"\"\"\n",
    "\n",
    "    xyz = [\"dom_x\", \"dom_y\", \"dom_z\"]\n",
    "    string_id_column = \"string\"\n",
    "    sensor_id_column = \"sensor_id\"\n",
    "\n",
    "    def feature_map(self) -> Dict[str, Callable]:\n",
    "        return {\n",
    "            \"dom_x\": self._xyz,\n",
    "            \"dom_y\": self._xyz,\n",
    "            \"dom_z\": self._xyz,\n",
    "            \"dom_time\": self._time,\n",
    "            \"charge\": self._charge,\n",
    "        }\n",
    "\n",
    "    def _xyz(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x  # x / 500.0\n",
    "\n",
    "    def _time(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x  # (x - 1.0e4) / 3.0e4\n",
    "\n",
    "    def _charge(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x  # torch.log10(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab3b24-b98c-4e7a-8f7b-08c70989ec68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc963bc-e30f-4dff-9946-fa1a97ef835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 06:11:03] ==> Starting section: 1) Data Representation (Graph Definition)\n",
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[32mINFO    \u001b[0m 2026-01-05 06:11:03 - PONE.__init__ - Writing log to \u001b[1mlogs/graphnet_20260105-061103.log\u001b[0m\n",
      "KNN configuration: K=8 (node = OM via ClusterSummaryFeatures)\n"
     ]
    }
   ],
   "source": [
    "## 1) Data Representation (Graph Definition)\n",
    "\n",
    "section_banner(\"1) Data Representation (Graph Definition)\")\n",
    "\n",
    "FEATURES = [\"dom_x\", \"dom_y\", \"dom_z\", \"dom_time\", \"charge\"]\n",
    "detector = PONE(replace_with_identity=FEATURES)\n",
    "\n",
    "K = 8\n",
    "print(f\"KNN configuration: K={K} (node = OM via ClusterSummaryFeatures)\")\n",
    "\n",
    "node_definition = ClusterSummaryFeatures(\n",
    "    cluster_on=[\"dom_x\", \"dom_y\", \"dom_z\"],\n",
    "    input_feature_names=FEATURES,\n",
    "    charge_label=\"charge\",\n",
    "    time_label=\"dom_time\",\n",
    "    add_counts=True,\n",
    ")\n",
    "\n",
    "data_representation = KNNGraph(\n",
    "    detector=detector,\n",
    "    node_definition=node_definition,\n",
    "    nb_nearest_neighbours=K,\n",
    "    columns=[0, 1, 2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866f460a-8856-4b6f-9cb4-c9eac5f6c27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNGraph(\n",
       "  KNNGraph(\n",
       "  {\n",
       "      'arguments': {\n",
       "          'detector': ModelConfig(\n",
       "  {\n",
       "      'replace_with_identity': ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge'],\n",
       "  }\n",
       "  ),\n",
       "          'node_definition': ModelConfig(\n",
       "  {\n",
       "      'cluster_on': ['dom_x', 'dom_y', 'dom_z'],\n",
       "      'input_feature_names': ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge'],\n",
       "      'charge_label': 'charge',\n",
       "      'time_label': 'dom_time',\n",
       "      'total_charge': True,\n",
       "      'charge_after_t': [10, 50, 100],\n",
       "      'time_of_first_hit': True,\n",
       "      'time_spread': True,\n",
       "      'time_std': True,\n",
       "      'time_after_charge_pct': [1, 3, 5, 11, 15, 20, 50, 80],\n",
       "      'charge_standardization': 'log',\n",
       "      'time_standardization': 0.001,\n",
       "      'order_in_time': True,\n",
       "      'add_counts': True,\n",
       "  }\n",
       "  ),\n",
       "          'input_feature_names': None,\n",
       "          'dtype': torch.float32,\n",
       "          'perturbation_dict': None,\n",
       "          'seed': None,\n",
       "          'nb_nearest_neighbours': 8,\n",
       "          'columns': [0, 1, 2],\n",
       "          'distance_as_edge_feature': False,\n",
       "      },\n",
       "  })\n",
       "  (_detector): PONE(\n",
       "    PONE(\n",
       "    {\n",
       "        'arguments': {\n",
       "            'replace_with_identity': ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge'],\n",
       "        },\n",
       "    })\n",
       "  )\n",
       "  (_node_definition): ClusterSummaryFeatures(\n",
       "    ClusterSummaryFeatures(\n",
       "    {\n",
       "        'arguments': {\n",
       "            'cluster_on': ['dom_x', 'dom_y', 'dom_z'],\n",
       "            'input_feature_names': ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge'],\n",
       "            'charge_label': 'charge',\n",
       "            'time_label': 'dom_time',\n",
       "            'total_charge': True,\n",
       "            'charge_after_t': [10, 50, 100],\n",
       "            'time_of_first_hit': True,\n",
       "            'time_spread': True,\n",
       "            'time_std': True,\n",
       "            'time_after_charge_pct': [1, 3, 5, 11, 15, 20, 50, 80],\n",
       "            'charge_standardization': 'log',\n",
       "            'time_standardization': 0.001,\n",
       "            'order_in_time': True,\n",
       "            'add_counts': True,\n",
       "        },\n",
       "    })\n",
       "  )\n",
       "  (_edge_definition): KNNEdges(\n",
       "    KNNEdges(\n",
       "    {\n",
       "        'arguments': {\n",
       "            'nb_nearest_neighbours': 8,\n",
       "            'columns': [0, 1, 2],\n",
       "        },\n",
       "    })\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bb260e-aa9b-49d7-a205-79796cdaeb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 06:11:03] ==> Starting section: 2) Dataset (Parquet -> PyG Data)\n",
      "Dataset config:\n",
      "  path: /project/def-nahee/kbas/POM_Response_Parquet/merged\n",
      "  pulsemaps: features\n",
      "  truth_table: truth\n",
      "  features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge']\n",
      "  truth: ['energy']\n",
      "Dataset created.\n"
     ]
    }
   ],
   "source": [
    "## 2) Dataset (Parquet -> PyG Data)\n",
    "\n",
    "section_banner(\"2) Dataset (Parquet -> PyG Data)\")\n",
    "\n",
    "dataset_kwargs = dict(\n",
    "    path=str(PARQUET_ROOT),\n",
    "    pulsemaps=PULSEMAPS,\n",
    "    truth_table=TRUTH_TABLE,\n",
    "    features=FEATURES,\n",
    "    truth=TRUTH_LABELS,\n",
    ")\n",
    "\n",
    "print(\"Dataset config:\")\n",
    "print(\"  path:\", dataset_kwargs[\"path\"])\n",
    "print(\"  pulsemaps:\", dataset_kwargs[\"pulsemaps\"])\n",
    "print(\"  truth_table:\", dataset_kwargs[\"truth_table\"])\n",
    "print(\"  features:\", dataset_kwargs[\"features\"])\n",
    "print(\"  truth:\", dataset_kwargs[\"truth\"])\n",
    "\n",
    "dataset = ParquetDataset(**dataset_kwargs, data_representation=data_representation)\n",
    "print(\"Dataset created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb54ee8-2e46-4683-9115-3253575a0935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 06:11:03] ==> Starting section: 3) DataLoader\n",
      "DataLoader config: batch_size=4, num_workers=2, shuffle=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6061446/kbas/pone_offline/graphnet_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## 3) DataLoader\n",
    "\n",
    "section_banner(\"3) DataLoader\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(f\"DataLoader config: batch_size={BATCH_SIZE}, num_workers={NUM_WORKERS}, shuffle=True\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba0be4d-6f19-47ce-ad5e-0706ad194583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 06:11:03] ==> Starting section: 4) Backbone\n",
      "Backbone: DynEdge\n"
     ]
    }
   ],
   "source": [
    "## 4) Backbone\n",
    "\n",
    "section_banner(\"4) Backbone\")\n",
    "\n",
    "print(\"Backbone: DynEdge\")\n",
    "backbone = DynEdge(\n",
    "    nb_inputs=data_representation.nb_outputs,\n",
    "    global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0554835-3a7d-4bfe-994d-18c26a87b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 06:11:03] ==> Starting section: 5) Task: Energy Reconstruction\n",
      "Task: EnergyReconstruction (target_labels=['energy'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 5) Task: Energy Reconstruction (event-level)\n",
    "\n",
    "section_banner(\"5) Task: Energy Reconstruction\")\n",
    "\n",
    "ENERGY_LABEL = \"energy\"\n",
    "print(f\"Task: EnergyReconstruction (target_labels={[ENERGY_LABEL]})\")\n",
    "\n",
    "task = EnergyReconstruction(\n",
    "    hidden_size=backbone.nb_outputs,\n",
    "    target_labels=[ENERGY_LABEL],\n",
    "    loss_function=LogCoshLoss(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9777e758-377f-4b31-9d03-6aa01ec6e419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 06:11:03] ==> Starting section: 6) StandardModel\n",
      "Building StandardModel ...\n",
      "Model created.\n"
     ]
    }
   ],
   "source": [
    "## 6) StandardModel\n",
    "\n",
    "section_banner(\"6) StandardModel\")\n",
    "\n",
    "print(\"Building StandardModel ...\")\n",
    "model = StandardModel(\n",
    "    tasks=[task],\n",
    "    data_representation=data_representation,\n",
    "    backbone=backbone,\n",
    ")\n",
    "print(\"Model created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea477cb-fd0b-4068-818d-5270d1c4737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 06:11:03] ==> Starting section: 7) Train\n",
      "Starting training: max_epochs=5, gpus=1\n",
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[32mINFO    \u001b[0m 2026-01-05 06:11:03 - StandardModel._print_callbacks - Training initiated with callbacks: ProgressBar\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6061446/kbas/pone_offline/graphnet_env/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /project/6061446/kbas/pone_offline/graphnet_env/lib/ ...\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/project/6061446/kbas/pone_offline/graphnet_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/project/6061446/kbas/pone_offline/graphnet_env/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-6b0e6fa0-a347-51dc-b588-5de81ade7f9c]\n",
      "\n",
      "  | Name                 | Type       | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | _tasks               | ModuleList | 129    | train\n",
      "1 | _data_representation | KNNGraph   | 0      | train\n",
      "2 | backbone             | DynEdge    | 1.4 M  | train\n",
      "------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.455     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54273a771feb48f9ab886c4b94afba9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "## 7) Train\n",
    "\n",
    "section_banner(\"7) Train\")\n",
    "\n",
    "MAX_EPOCHS = 5\n",
    "GPUS = 1 if torch.cuda.is_available() else None\n",
    "print(f\"Starting training: max_epochs={MAX_EPOCHS}, gpus={GPUS}\")\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=None,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gpus=GPUS,\n",
    "    distribution_strategy=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa479c-00b7-44e2-998b-867e3fdc9218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graphnet_env)",
   "language": "python",
   "name": "graphnet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
